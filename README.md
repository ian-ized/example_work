# example_work
The first jupiter notebook is a file used in my most recent project at the Golden Lab. It combines the schedule of the experiment,
positional data from a machine learning model trained by many late nights annotating, and the video itself to visualize the 
subject, its points of interest, and its gaze. The visualizaition was made using this data. The second uses opencv2 to isolate the
locations of the five gates in the five choice task. The two images of mice show the windows isolated automatically using the isolate
windows code, and the pose visual image shows a screenshot form a full video, which shows estimated gaze as well as positional data.
